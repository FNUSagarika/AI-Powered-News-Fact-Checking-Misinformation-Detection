{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4Wu_aEWcGNHT",
        "outputId": "b8616406-0c6e-4b75-8de9-43d8b7c8727a"
      },
      "outputs": [],
      "source": [
        "pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iDvQ5jagGM0f",
        "outputId": "fad4065b-da17-4d62-e10c-4cb660aea5c5"
      },
      "outputs": [],
      "source": [
        " pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IlGGlqZjGWAx",
        "outputId": "f3939d18-ce7c-4918-c5c6-7ffc5fab83dd"
      },
      "outputs": [],
      "source": [
        " pip install langchain-pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bCwuX-JGGYmH",
        "outputId": "018c3041-e3f9-4b43-a0a3-8f63341a8346"
      },
      "outputs": [],
      "source": [
        "pip install langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "A2W5xu2BGd21",
        "outputId": "005c3e2e-2986-456e-c2d2-eac7a747de38"
      },
      "outputs": [],
      "source": [
        " pip install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "B17PBdphGhuO",
        "outputId": "cf9b185c-22c3-4b12-e04c-c4b6809e2e72"
      },
      "outputs": [],
      "source": [
        "pip install pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tbvKIgsoGlse",
        "outputId": "70c1ae0e-25ab-4256-e1d5-53f0b4eaf0f7"
      },
      "outputs": [],
      "source": [
        "pip install openai-chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zvcDj-5iGoFS",
        "outputId": "7521fdd5-aac1-4760-9ad9-33e4dd9caf9d"
      },
      "outputs": [],
      "source": [
        "pip install google-search-results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ilQ7c2rIGqk5",
        "outputId": "aae18212-7e6d-40f0-d0d5-e6a77c2d58a3"
      },
      "outputs": [],
      "source": [
        "pip uninstall pinecone-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAYxeXA1Gt9W"
      },
      "outputs": [],
      "source": [
        "pip install pinecone-client>=3.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JdwHon1yGw_V",
        "outputId": "13fa334c-5495-4630-b324-0de6ea532933"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet langchain openai pinecone-client==2.2.1 python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "B-vb_IJ1G2Of",
        "outputId": "76cadd01-9fbd-4c9a-f6f2-962d4e7cbd28"
      },
      "outputs": [],
      "source": [
        "pip install --upgrade pinecone-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6eRopJ1G3mh"
      },
      "outputs": [],
      "source": [
        "# All imports\n",
        "\n",
        "import os\n",
        "import requests\n",
        "#import pinecone\n",
        "\n",
        "#from pinecone import Pinecone, ServerlessSpec                         # rel: Start\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings              # rel: Step 2\n",
        "from langchain.document_loaders import PyPDFLoader                    # rel: Step 3\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter    # rel: Step 3\n",
        "from langchain.vectorstores import Pinecone as LC_Pinecone            # rel: Step 4\n",
        "from langchain_openai import OpenAI                                   # rel: Step 5\n",
        "from langchain.chains import RetrievalQA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OU95QU-hLMH-",
        "outputId": "40a4a3aa-e53b-4858-963f-8e0bba38d523"
      },
      "outputs": [],
      "source": [
        "!pip install langchain openai wikipedia faiss-cpu scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "nFpc9C_7LQbw",
        "outputId": "e9a1dc17-8bbe-4fc4-9f10-15af1ed074f0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-61b15332-ff7f-41e2-81a2-81d8eea042b0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-61b15332-ff7f-41e2-81a2-81d8eea042b0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving train.tsv to train.tsv\n",
            "Saving test.tsv to test.tsv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GN-qB0l5G9iL"
      },
      "outputs": [],
      "source": [
        "# Set up API keys and environment variables\n",
        "\n",
        "OPENAI_API_KEY = \" your api key here \"\n",
        "PINECONE_KEY = \"your api key here\"\n",
        "PINECONE_REGION = \"your region here\"\n",
        "\n",
        "os.environ[\"PINECONE_KEY\"] = PINECONE_KEY\n",
        "os.environ[\"PINECONE_REGION\"] = PINECONE_REGION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgCva8JZWo8c"
      },
      "outputs": [],
      "source": [
        "# ========================\n",
        "# STEP 4: Load & preprocess LIAR dataset\n",
        "# ========================\n",
        "import pandas as pd\n",
        "\n",
        "def load_liar_dataset(file_path):\n",
        "    df = pd.read_csv(file_path, sep='\\t', header=None, names=[\n",
        "        \"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"job_title\",\n",
        "        \"state\", \"party\", \"barely_true_counts\", \"false_counts\", \"half_true_counts\",\n",
        "        \"mostly_true_counts\", \"pants_on_fire_counts\", \"context\"\n",
        "    ])\n",
        "    return df[[\"statement\", \"label\"]]\n",
        "\n",
        "def preprocess_labels(df):\n",
        "    binary_map = {\n",
        "        'false': 'fake',\n",
        "        'pants-fire': 'fake',\n",
        "        'barely-true': 'fake',\n",
        "        'half-true': 'real',\n",
        "        'mostly-true': 'real',\n",
        "        'true': 'real'\n",
        "    }\n",
        "    df['label'] = df['label'].map(binary_map)\n",
        "    return df[df['label'].isin(['fake', 'real'])]\n",
        "\n",
        "def get_train_test_split(train_path, test_path):\n",
        "    train_df = preprocess_labels(load_liar_dataset(train_path))\n",
        "    test_df = preprocess_labels(load_liar_dataset(test_path))\n",
        "    return train_df, test_df\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PhFOZ5EWkQe"
      },
      "outputs": [],
      "source": [
        "# ========================\n",
        "# STEP 5: Build Wikipedia RAG Vector Store\n",
        "# ========================\n",
        "from langchain.document_loaders import WikipediaLoader\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "def build_wiki_vector_store(search_terms, api_key):\n",
        "    documents = []\n",
        "    for term in search_terms:\n",
        "        loader = WikipediaLoader(query=term)\n",
        "        docs = loader.load()\n",
        "        documents.extend(docs)\n",
        "\n",
        "    embedding = OpenAIEmbeddings(openai_api_key=api_key)\n",
        "    vectorstore = FAISS.from_documents(documents, embedding)\n",
        "    return vectorstore\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZA4TvpBWhiQ"
      },
      "outputs": [],
      "source": [
        "# ========================\n",
        "# STEP 6: Setup LangChain QA\n",
        "# ========================\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "def setup_qa(vectorstore, api_key):\n",
        "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, openai_api_key=api_key)\n",
        "    return RetrievalQA.from_chain_type(llm=llm, retriever=vectorstore.as_retriever())\n",
        "\n",
        "def ask_model(qa, claim):\n",
        "    prompt = f\"Is the following claim likely fake or real? Just say 'fake' or 'real'. Claim: {claim}\"\n",
        "    response = qa.run(prompt)\n",
        "    return \"fake\" if \"fake\" in response.lower() else \"real\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3ZwJHRWWd3X"
      },
      "outputs": [],
      "source": [
        "# ========================\n",
        "# STEP 7: Evaluation Metrics\n",
        "# ========================\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def evaluate(y_true, y_pred):\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"precision\": precision_score(y_true, y_pred, pos_label=\"fake\"),\n",
        "        \"recall\": recall_score(y_true, y_pred, pos_label=\"fake\"),\n",
        "        \"f1_score\": f1_score(y_true, y_pred, pos_label=\"fake\")\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJGzxA8BWatz",
        "outputId": "f879c25c-4c4b-4558-ba03-1193392f7680"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.11/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n",
            "<ipython-input-20-bad5f12ac380>:15: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
            "  embedding = OpenAIEmbeddings(openai_api_key=api_key)\n",
            "<ipython-input-21-e675808729f4>:8: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, openai_api_key=api_key)\n",
            "<ipython-input-21-e675808729f4>:13: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = qa.run(prompt)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Evaluation Metrics:\n",
            "Accuracy: 0.70\n",
            "Precision: 0.73\n",
            "Recall: 0.69\n",
            "F1_score: 0.71\n"
          ]
        }
      ],
      "source": [
        "# STEP 8: Run RAG Pipeline on LIAR dataset\n",
        "# ========================\n",
        "\n",
        "# Only run this if you uploaded `train.tsv` and `test.tsv`\n",
        "try:\n",
        "    train_df, test_df = get_train_test_split(\"train.tsv\", \"test.tsv\")\n",
        "    search_terms = [\"Fake news\", \"Misinformation\", \"COVID-19 misinformation\", \"Political disinformation\", \"Social media\"]\n",
        "    vectorstore = build_wiki_vector_store(search_terms, OPENAI_API_KEY)\n",
        "    qa = setup_qa(vectorstore, OPENAI_API_KEY)\n",
        "\n",
        "    sample_test = test_df.sample(30, random_state=42)\n",
        "    y_true = sample_test['label'].tolist()\n",
        "    y_pred = [ask_model(qa, claim) for claim in sample_test['statement']]\n",
        "\n",
        "    metrics = evaluate(y_true, y_pred)\n",
        "\n",
        "    print(\"\\nüìä Evaluation Metrics:\")\n",
        "    for k, v in metrics.items():\n",
        "        print(f\"{k.capitalize()}: {v:.2f}\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è Skipping LIAR dataset pipeline ‚Äî train.tsv/test.tsv not uploaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8ADIGJGV5Qfs",
        "outputId": "a617859e-2574-4c7e-aaaf-3c869c4f44c6"
      },
      "outputs": [],
      "source": [
        "!pip install newspaper3k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWARwQSA5YIN",
        "outputId": "1f2cc448-ac24-4ff8-e6de-c5f56b66e767"
      },
      "outputs": [],
      "source": [
        "!pip install lxml_html_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kcf3lpkdWW9r"
      },
      "outputs": [],
      "source": [
        "# ========================\n",
        "# STEP 9: URL-based Fake News Checker (on-demand)\n",
        "# ========================\n",
        "from newspaper import Article\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "def extract_text_from_url(url):\n",
        "    article = Article(url)\n",
        "    article.download()\n",
        "    article.parse()\n",
        "    return article.text\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, openai_api_key=OPENAI_API_KEY)\n",
        "\n",
        "TEMPLATE = \"\"\"\n",
        "You are a fact-checking assistant. Assess the credibility of the following content:\n",
        "\n",
        "\\\"\\\"\\\"{content}\\\"\\\"\\\"\n",
        "\n",
        "Respond with one of the following labels: \"credible\", \"potentially misleading\", or \"fake\".\n",
        "Then briefly explain why.\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(TEMPLATE)\n",
        "\n",
        "def analyze_text(text):\n",
        "    input_prompt = prompt.format(content=text[:2000])  # Truncate to fit context window\n",
        "    response = llm.predict(input_prompt)\n",
        "\n",
        "    if \"credible\" in response.lower():\n",
        "        return \"credible\", response\n",
        "    elif \"misleading\" in response.lower():\n",
        "        return \"potentially misleading\", response\n",
        "    else:\n",
        "        return \"fake\", response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HIdV2tIWWDc",
        "outputId": "a03c2c91-620b-4164-db52-0390e790ac32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Paste a URL to analyze: https://www.snopes.com/fact-check/clinton-votes-found-in-warehouse/\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-28-66885b567865>:28: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = llm.predict(input_prompt)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üßæ Verdict: FAKE\n",
            "\n",
            "üí¨ Explanation:\n",
            "\n",
            "Label: Fake\n",
            "\n",
            "Explanation: The content contains false information about pre-marked ballots for Hillary Clinton and other Democratic candidates being found in a warehouse in Ohio. The article is based on fabricated information and altered images, making it completely unreliable and fake. Additionally, the Christian Times Newspaper has a history of promoting conspiracy theories and false stories, further diminishing its credibility.\n"
          ]
        }
      ],
      "source": [
        "# ========================\n",
        "# STEP 10: Run on URL\n",
        "# ========================\n",
        "url = input(\"üì• Paste a URL to analyze: \").strip()\n",
        "try:\n",
        "    article_text = extract_text_from_url(url)\n",
        "    verdict, explanation = analyze_text(article_text)\n",
        "\n",
        "    print(f\"\\nüßæ Verdict: {verdict.upper()}\\n\")\n",
        "    print(\"üí¨ Explanation:\\n\")\n",
        "    print(explanation)\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error analyzing URL: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTZY2GVJ5l2Z",
        "outputId": "f4d3c937-5e9c-494b-a1ee-2e17759e8b32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Paste a URL to analyze: https://www.nytimes.com/2015/08/16/us/politics/att-helped-nsa-spy-on-an-array-of-internet-traffic.html?mtrref=www.nytimes.com&gwh=39731CD5654384DA101033E4C386CB44&gwt=pay\n",
            "\n",
            "üßæ Verdict: POTENTIALLY MISLEADING\n",
            "\n",
            "üí¨ Explanation:\n",
            "\n",
            "Potentially misleading\n",
            "\n",
            "While the content provides specific details and references to support its claims, it is important to note that the information is based on leaked N.S.A. documents and anonymous sources. Without official confirmation or verification from the government or telecom companies involved, the claims made in the content could be potentially misleading.\n"
          ]
        }
      ],
      "source": [
        "# ========================\n",
        "# STEP 10: Run on URL\n",
        "# ========================\n",
        "url = input(\"üì• Paste a URL to analyze: \").strip()\n",
        "try:\n",
        "    article_text = extract_text_from_url(url)\n",
        "    verdict, explanation = analyze_text(article_text)\n",
        "\n",
        "    print(f\"\\nüßæ Verdict: {verdict.upper()}\\n\")\n",
        "    print(\"üí¨ Explanation:\\n\")\n",
        "    print(explanation)\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error analyzing URL: {e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
